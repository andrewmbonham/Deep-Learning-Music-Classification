{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Classification via deep neural network\n",
    "#### **Overview:** This notebook serves to analyze music audio files labeled by genre and develop a deep neural network to predict the music genre based on numerical features of the audio file. \n",
    "## Contents\n",
    "### **Generate Features**\n",
    "#### **30 second segments**\n",
    "#### **3 second segments**\n",
    "### **Modeling the Data** (Run from here if you have the CSV file(s))\n",
    "#### **30 second segments**\n",
    "##### Read and process the data\n",
    "##### Split into training and test data\n",
    "##### Set up the model and metrics analysis \n",
    "##### Train the model\n",
    "##### Results\n",
    "#### **3 second segments**\n",
    "##### Split into training and test data\n",
    "##### Set up the model\n",
    "##### Train the model\n",
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 second segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segment = 1\n",
    "num_mfcc = 20\n",
    "sample_rate = 22050\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "my_csv = {\n",
    "    \"filename\": [],\n",
    "    \"chroma_stft_mean\": [],\n",
    "    \"chroma_stft_var\": [],\n",
    "    \"rms_mean\": [],\n",
    "    \"rms_var\": [],\n",
    "    \"spectral_centroid_mean\": [],\n",
    "    \"spectral_centroid_var\": [],\n",
    "    \"spectral_bandwidth_mean\": [],\n",
    "    \"spectral_bandwidth_var\": [],\n",
    "    \"rolloff_mean\": [],\n",
    "    \"rolloff_var\": [],\n",
    "    \"zero_crossing_rate_mean\": [],\n",
    "    \"zero_crossing_rate_var\": [],\n",
    "    \"harmony_mean\": [],\n",
    "    \"harmony_var\": [],\n",
    "    \"perceptr_mean\": [],\n",
    "    \"perceptr_var\": [],\n",
    "    \"tempo\": [],\n",
    "    \"mfcc1_mean\": [],\n",
    "    \"mfcc1_var\": [],\n",
    "    \"mfcc2_mean\": [],\n",
    "    \"mfcc2_var\": [],\n",
    "    \"mfcc3_mean\": [],\n",
    "    \"mfcc3_var\": [],\n",
    "    \"mfcc4_mean\": [],\n",
    "    \"mfcc4_var\": [],\n",
    "    \"mfcc5_mean\": [],\n",
    "    \"mfcc5_var\": [],\n",
    "    \"mfcc6_mean\": [],\n",
    "    \"mfcc6_var\": [],\n",
    "    \"mfcc7_mean\": [],\n",
    "    \"mfcc7_var\": [],\n",
    "    \"mfcc8_mean\": [],\n",
    "    \"mfcc8_var\": [],\n",
    "    \"mfcc9_mean\": [],\n",
    "    \"mfcc9_var\": [],\n",
    "    \"mfcc10_mean\": [],\n",
    "    \"mfcc10_var\": [],\n",
    "    \"mfcc11_mean\": [],\n",
    "    \"mfcc11_var\": [],\n",
    "    \"mfcc12_mean\": [],\n",
    "    \"mfcc12_var\": [],\n",
    "    \"mfcc13_mean\": [],\n",
    "    \"mfcc13_var\": [],\n",
    "    \"mfcc14_mean\": [],\n",
    "    \"mfcc14_var\": [],\n",
    "    \"mfcc15_mean\": [],\n",
    "    \"mfcc15_var\": [],\n",
    "    \"mfcc16_mean\": [],\n",
    "    \"mfcc16_var\": [],\n",
    "    \"mfcc17_mean\": [],\n",
    "    \"mfcc17_var\": [],\n",
    "    \"mfcc18_mean\": [],\n",
    "    \"mfcc18_var\": [],\n",
    "    \"mfcc19_mean\": [],\n",
    "    \"mfcc19_var\": [],\n",
    "    \"mfcc20_mean\": [],\n",
    "    \"mfcc20_var\": [],\n",
    "    \"label\": [],\n",
    "}\n",
    "my_3_csv = my_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"genres\"\n",
    "audio_files = glob(dataset_path + \"/*/*\")\n",
    "genre = glob(dataset_path + \"/*\")\n",
    "n_genres = len(genre)\n",
    "genre = [genre[x].split(\"/\")[-1] for x in range(n_genres) if \".mf\" not in genre[x]]\n",
    "print(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = \"\"\n",
    "for f in sorted(audio_files):\n",
    "    if genre != f.split(\"/\")[-2]:\n",
    "        genre = f.split(\"/\")[-2]\n",
    "        print(\"Procesassing \" + genre + \"...\")\n",
    "    fname = f.split(\"/\")[-1]\n",
    "    try:\n",
    "        y, sr = librosa.load(f, sr=sample_rate)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # Chromagram\n",
    "    chroma_hop_length = 512  # 5000?\n",
    "    chromagram = librosa.feature.chroma_stft(\n",
    "        y=y, sr=sample_rate, hop_length=chroma_hop_length\n",
    "    )\n",
    "    my_csv[\"chroma_stft_mean\"].append(chromagram.mean())\n",
    "    my_csv[\"chroma_stft_var\"].append(chromagram.var())\n",
    "\n",
    "    # Root Mean Square Energy\n",
    "    RMSEn = librosa.feature.rms(y=y)\n",
    "    my_csv[\"rms_mean\"].append(RMSEn.mean())\n",
    "    my_csv[\"rms_var\"].append(RMSEn.var())\n",
    "\n",
    "    # Spectral Centroid\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y)\n",
    "    my_csv[\"spectral_centroid_mean\"].append(spec_cent.mean())\n",
    "    my_csv[\"spectral_centroid_var\"].append(spec_cent.var())\n",
    "\n",
    "    # Spectral Bandwith\n",
    "    spec_band = librosa.feature.spectral_bandwidth(y=y, sr=sample_rate)\n",
    "    my_csv[\"spectral_bandwidth_mean\"].append(spec_band.mean())\n",
    "    my_csv[\"spectral_bandwidth_var\"].append(spec_band.var())\n",
    "\n",
    "    # Rolloff\n",
    "    spec_roll = librosa.feature.spectral_rolloff(y=y, sr=sample_rate)\n",
    "    my_csv[\"rolloff_mean\"].append(spec_roll.mean())\n",
    "    my_csv[\"rolloff_var\"].append(spec_roll.var())\n",
    "\n",
    "    # Zero Crossing Rate\n",
    "    zero_crossing = librosa.feature.zero_crossing_rate(y=y)\n",
    "    my_csv[\"zero_crossing_rate_mean\"].append(zero_crossing.mean())\n",
    "    my_csv[\"zero_crossing_rate_var\"].append(zero_crossing.var())\n",
    "\n",
    "    # Harmonics and Perceptrual\n",
    "    harmony, perceptr = librosa.effects.hpss(y=y)\n",
    "    my_csv[\"harmony_mean\"].append(harmony.mean())\n",
    "    my_csv[\"harmony_var\"].append(harmony.var())\n",
    "    my_csv[\"perceptr_mean\"].append(perceptr.mean())\n",
    "    my_csv[\"perceptr_var\"].append(perceptr.var())\n",
    "\n",
    "    # Tempo\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    my_csv[\"tempo\"].append(tempo)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=y, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length\n",
    "    )\n",
    "    mfcc = mfcc.T\n",
    "\n",
    "    my_csv[\"filename\"].append(fname)\n",
    "    my_csv[\"label\"].append(f.split(\"/\")[-2])\n",
    "    for x in range(20):\n",
    "        feat1 = \"mfcc\" + str(x + 1) + \"_mean\"\n",
    "        feat2 = \"mfcc\" + str(x + 1) + \"_var\"\n",
    "        my_csv[feat1].append(mfcc[:, x].mean())\n",
    "        my_csv[feat2].append(mfcc[:, x].var())\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(my_csv)\n",
    "df.to_csv(\"myfeatures.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 second segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mfcc = 20\n",
    "sample_rate = 22050\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "num_segment = 10\n",
    "samples_per_segment = int(sample_rate * 30 / num_segment)\n",
    "dataset_path = \"genres\"\n",
    "audio_files = glob(dataset_path + \"/*/*\")\n",
    "genre = glob(dataset_path + \"/*\")\n",
    "n_genres = len(genre)\n",
    "genre = [genre[x].split(\"/\")[-1] for x in range(n_genres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segment = 10\n",
    "\n",
    "samples_per_segment = int(sample_rate * 30 / num_segment)\n",
    "genre = \"\"\n",
    "for f in sorted(audio_files):\n",
    "    if genre != f.split(\"/\")[-2]:\n",
    "        genre = f.split(\"/\")[-2]\n",
    "        print(\"Procesassing \" + genre + \"...\")\n",
    "    fname = f.split(\"/\")[-1]\n",
    "    # print(fname)\n",
    "    try:\n",
    "        y, sr = librosa.load(f, sr=sample_rate)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for n in range(num_segment):\n",
    "        y_seg = y[samples_per_segment * n : samples_per_segment * (n + 1)]\n",
    "        # Chromagram\n",
    "        chroma_hop_length = 512\n",
    "        chromagram = librosa.feature.chroma_stft(\n",
    "            y=y_seg, sr=sample_rate, hop_length=chroma_hop_length\n",
    "        )\n",
    "        my_3_csv[\"chroma_stft_mean\"].append(chromagram.mean())\n",
    "        my_3_csv[\"chroma_stft_var\"].append(chromagram.var())\n",
    "\n",
    "        # Root Mean Square Energy\n",
    "        RMSEn = librosa.feature.rms(y=y_seg)\n",
    "        my_3_csv[\"rms_mean\"].append(RMSEn.mean())\n",
    "        my_3_csv[\"rms_var\"].append(RMSEn.var())\n",
    "\n",
    "        # Spectral Centroid\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y_seg)\n",
    "        my_3_csv[\"spectral_centroid_mean\"].append(spec_cent.mean())\n",
    "        my_3_csv[\"spectral_centroid_var\"].append(spec_cent.var())\n",
    "\n",
    "        # Spectral Bandwith\n",
    "        spec_band = librosa.feature.spectral_bandwidth(y=y_seg, sr=sample_rate)\n",
    "        my_3_csv[\"spectral_bandwidth_mean\"].append(spec_band.mean())\n",
    "        my_3_csv[\"spectral_bandwidth_var\"].append(spec_band.var())\n",
    "\n",
    "        # Rolloff\n",
    "        spec_roll = librosa.feature.spectral_rolloff(y=y_seg, sr=sample_rate)\n",
    "        my_3_csv[\"rolloff_mean\"].append(spec_roll.mean())\n",
    "        my_3_csv[\"rolloff_var\"].append(spec_roll.var())\n",
    "\n",
    "        # Zero Crossing Rate\n",
    "        zero_crossing = librosa.feature.zero_crossing_rate(y=y_seg)\n",
    "        my_3_csv[\"zero_crossing_rate_mean\"].append(zero_crossing.mean())\n",
    "        my_3_csv[\"zero_crossing_rate_var\"].append(zero_crossing.var())\n",
    "\n",
    "        # Harmonics and Perceptrual\n",
    "        harmony, perceptr = librosa.effects.hpss(y=y_seg)\n",
    "        my_3_csv[\"harmony_mean\"].append(harmony.mean())\n",
    "        my_3_csv[\"harmony_var\"].append(harmony.var())\n",
    "        my_3_csv[\"perceptr_mean\"].append(perceptr.mean())\n",
    "        my_3_csv[\"perceptr_var\"].append(perceptr.var())\n",
    "\n",
    "        # Tempo\n",
    "        tempo, _ = librosa.beat.beat_track(y=y_seg, sr=sample_rate)\n",
    "        my_3_csv[\"tempo\"].append(tempo)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=y_seg, sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "        mfcc = mfcc.T\n",
    "\n",
    "        fseg_name = \".\".join(fname.split(\".\")[:2]) + f\".{n}.wav\"\n",
    "        my_3_csv[\"filename\"].append(fseg_name)\n",
    "        my_3_csv[\"label\"].append(genre)\n",
    "        for x in range(20):\n",
    "            feat1 = \"mfcc\" + str(x + 1) + \"_mean\"\n",
    "            feat2 = \"mfcc\" + str(x + 1) + \"_var\"\n",
    "            my_3_csv[feat1].append(mfcc[:, x].mean())\n",
    "            my_3_csv[feat2].append(mfcc[:, x].var())\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(my_3_csv)\n",
    "df.to_csv(\"myfeatures_3_sec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the data (Run from here if you have the CSV file(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 second segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"myfeatures.csv\", index_col=None)\n",
    "df_orig = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isna().sum().sum() == 0:\n",
    "    print(\"No NaNs to clean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"filename\", \"label\"], axis=1)\n",
    "df = (df - df.mean()) / df.std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = LabelEncoder().fit_transform(df_orig[\"label\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby(\"label\").mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=df_orig[\"label\"].unique(), height=means.iloc[:, 0])\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.ylabel(means.columns[0] + \" (normalized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model and metrics analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(input_shape):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(512, activation=\"relu\", input_shape=input_shape),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, optimizer):\n",
    "    batch_size = 128\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\"\n",
    "    )\n",
    "    return model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validate(history):\n",
    "    print(\"Max validation accuracy:\", round(max(history.history[\"val_accuracy\"]), 3))\n",
    "    pd.DataFrame(history.history).plot()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Model Performance\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],)\n",
    "model = generate_model(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = train_model(model=model, epochs=100, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"Test loss:\", round(test_loss, 3))\n",
    "print(\"Best test accuracy:\", round(test_acc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validate(model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Second Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"myfeatures_3_sec.csv\", index_col=None)\n",
    "df_orig = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df.isna().sum().sum() == 0:\n",
    "    print(\"No NaNs to clean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"filename\", \"label\"], axis=1)\n",
    "df = (df - df.mean()) / df.std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = LabelEncoder().fit_transform(df_orig[\"label\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby(\"label\").mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=df_orig[\"label\"].unique(), height=means.iloc[:, 0])\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.ylabel(means.columns[0] + \" (normalized)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[:, :-1])\n",
    "y = np.array(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],)\n",
    "model = generate_model(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = train_model(model=model, epochs=300, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"Test loss:\", round(test_loss, 3))\n",
    "print(\"Best test accuracy:\", round(test_acc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validate(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
